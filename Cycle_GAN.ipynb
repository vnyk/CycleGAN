{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cycle_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcyoFmSRkVgl",
        "colab_type": "code",
        "outputId": "df1ff6fb-fb96-4c65-ea2f-61cef051f6dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-13 08:19:00--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.72.208\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.72.208|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26421880 (25M) [binary/octet-stream]\n",
            "Saving to: ‘train-images-idx3-ubyte.gz.1’\n",
            "\n",
            "train-images-idx3-u 100%[===================>]  25.20M  11.8MB/s    in 2.1s    \n",
            "\n",
            "2020-04-13 08:19:02 (11.8 MB/s) - ‘train-images-idx3-ubyte.gz.1’ saved [26421880/26421880]\n",
            "\n",
            "--2020-04-13 08:19:03--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.72.208\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.72.208|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29515 (29K) [binary/octet-stream]\n",
            "Saving to: ‘train-labels-idx1-ubyte.gz.1’\n",
            "\n",
            "train-labels-idx1-u 100%[===================>]  28.82K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-04-13 08:19:03 (208 KB/s) - ‘train-labels-idx1-ubyte.gz.1’ saved [29515/29515]\n",
            "\n",
            "--2020-04-13 08:19:05--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.72.130\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.72.130|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4422102 (4.2M) [binary/octet-stream]\n",
            "Saving to: ‘t10k-images-idx3-ubyte.gz.1’\n",
            "\n",
            "t10k-images-idx3-ub 100%[===================>]   4.22M  3.78MB/s    in 1.1s    \n",
            "\n",
            "2020-04-13 08:19:06 (3.78 MB/s) - ‘t10k-images-idx3-ubyte.gz.1’ saved [4422102/4422102]\n",
            "\n",
            "--2020-04-13 08:19:07--  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Resolving fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)... 52.219.72.130\n",
            "Connecting to fashion-mnist.s3-website.eu-central-1.amazonaws.com (fashion-mnist.s3-website.eu-central-1.amazonaws.com)|52.219.72.130|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5148 (5.0K) [binary/octet-stream]\n",
            "Saving to: ‘t10k-labels-idx1-ubyte.gz.1’\n",
            "\n",
            "t10k-labels-idx1-ub 100%[===================>]   5.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-04-13 08:19:08 (581 MB/s) - ‘t10k-labels-idx1-ubyte.gz.1’ saved [5148/5148]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx26lBNnlSb-",
        "colab_type": "code",
        "outputId": "42b83821-17b6-4686-8150-9ee956667752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MNIST_Fashion\t\t     t10k-labels-idx1-ubyte.gz.1\n",
            "sample_data\t\t     train-images-idx3-ubyte.gz\n",
            "t10k-images-idx3-ubyte.gz    train-images-idx3-ubyte.gz.1\n",
            "t10k-images-idx3-ubyte.gz.1  train-labels-idx1-ubyte.gz\n",
            "t10k-labels-idx1-ubyte.gz    train-labels-idx1-ubyte.gz.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BazpF0bmJM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.ndimage.interpolation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMupEFAhmh6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training PArams\n",
        "learning_rate = 0.0002\n",
        "batch_size = 32\n",
        "epochs = 100000\n",
        "\n",
        "#Network params\n",
        "image_dimension = 784 #img sz is 28x28\n",
        "#Discriminator Nodes\n",
        "H_dim = 128\n",
        "\n",
        "def xavier_init(shape):\n",
        "  return tf.random_normal(shape = shape, stddev= 1./tf.sqrt(shape[0]/2.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyTw0PEzt-Xz",
        "colab_type": "code",
        "outputId": "e34e402d-5714-4d85-9893-194f46622be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hn90CdeTXRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define placeholders for external input\n",
        "\n",
        "X_A = tf.placeholder(tf.float32, shape = [None, image_dimension])\n",
        "X_B = tf.placeholder(tf.float32, shape = [None, image_dimension])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZT-GzeQtaQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define weights and biases for dictionaries for Discriminator A\n",
        "\n",
        "Disc_A_W = { \"disc_H\" : tf.Variable(xavier_init([image_dimension, H_dim])),\n",
        "             \"disc_final\": tf.Variable(xavier_init([H_dim, 1]))}\n",
        "\n",
        "Disc_A_Bias = { \"disc_H\" : tf.Variable(xavier_init([H_dim])),\n",
        "             \"disc_final\": tf.Variable(xavier_init([1]))}\n",
        "\n",
        "\n",
        "# Define weights and biases for dictionaries for Discriminator B\n",
        "\n",
        "Disc_B_W = { \"disc_H\" : tf.Variable(xavier_init([image_dimension, H_dim])),\n",
        "             \"disc_final\": tf.Variable(xavier_init([H_dim, 1]))}\n",
        "\n",
        "Disc_B_Bias = { \"disc_H\" : tf.Variable(xavier_init([H_dim])),\n",
        "             \"disc_final\": tf.Variable(xavier_init([1]))}\n",
        "\n",
        "# Define weights and biases for dictionaries for Generator transforming A to B\n",
        "\n",
        "Gen_AB_W = { \"Gen_H\" : tf.Variable(xavier_init([image_dimension, H_dim])),\n",
        "             \"Gen_final\": tf.Variable(xavier_init([H_dim, image_dimension]))} #784 due to output dimension of the image\n",
        "\n",
        "Gen_AB_Bias = { \"Gen_H\" : tf.Variable(xavier_init([H_dim])),\n",
        "             \"Gen_final\": tf.Variable(xavier_init([image_dimension]))}\n",
        "\n",
        "\n",
        "# Define weights and biases for dictionaries for Generator transforming B to A\n",
        "\n",
        "Gen_BA_W = { \"Gen_H\" : tf.Variable(xavier_init([image_dimension, H_dim])),\n",
        "             \"Gen_final\": tf.Variable(xavier_init([H_dim, image_dimension]))}\n",
        "\n",
        "Gen_BA_Bias = { \"Gen_H\" : tf.Variable(xavier_init([H_dim])),\n",
        "             \"Gen_final\": tf.Variable(xavier_init([image_dimension]))}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od--kA7nvAch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Disc_A(x):\n",
        "  D_hidden_layer = tf.nn.relu(tf.add(tf.matmul(x, Disc_A_W[\"disc_H\"]), Disc_A_Bias[\"disc_H\"]))\n",
        "  disc_output = (tf.add(tf.matmul(D_hidden_layer, Disc_A_W[\"disc_final\"]), Disc_A_Bias[\"disc_final\"]))\n",
        "  disc_prob_output = tf.nn.sigmoid(disc_output)\n",
        "  return disc_prob_output\n",
        "\n",
        "def Disc_B(x):\n",
        "  D_hidden_layer = tf.nn.relu(tf.add(tf.matmul(x, Disc_B_W[\"disc_H\"]), Disc_B_Bias[\"disc_H\"]))\n",
        "  disc_output = (tf.add(tf.matmul(D_hidden_layer, Disc_B_W[\"disc_final\"]), Disc_B_Bias[\"disc_final\"]))\n",
        "  disc_prob_output = tf.nn.sigmoid(disc_output)\n",
        "  return disc_prob_output\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96s5eXy8wELi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generator NW\n",
        "def GenAB(x):\n",
        "  G_hidden_layer = tf.nn.relu(tf.add(tf.matmul(x, Gen_AB_W[\"Gen_H\"]), Gen_AB_Bias[\"Gen_H\"]))\n",
        "  Gen_output = (tf.add(tf.matmul(G_hidden_layer, Gen_AB_W[\"Gen_final\"]), Gen_AB_Bias[\"Gen_final\"]))\n",
        "  Gen_prob_output = tf.nn.sigmoid(Gen_output)\n",
        "  return Gen_prob_output\n",
        "\n",
        "def GenBA(x):\n",
        "  G_hidden_layer = tf.nn.relu(tf.add(tf.matmul(x, Gen_BA_W[\"Gen_H\"]), Gen_BA_Bias[\"Gen_H\"]))\n",
        "  Gen_output = (tf.add(tf.matmul(G_hidden_layer, Gen_BA_W[\"Gen_final\"]), Gen_BA_Bias[\"Gen_final\"]))\n",
        "  Gen_prob_output = tf.nn.sigmoid(Gen_output)\n",
        "  return Gen_prob_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfq_esohxQOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building Cycle GAN NW\n",
        "\n",
        "# GAN for approximating A Distribution\n",
        "\n",
        "X_BA = GenBA(X_B)\n",
        "Disc_A_real = Disc_A(X_A)\n",
        "Disc_A_fake = Disc_A(X_BA)\n",
        "\n",
        "# GAN for approximating B Distribution\n",
        "X_AB = GenBA(X_A)\n",
        "Disc_B_real = Disc_A(X_B)\n",
        "Disc_B_fake = Disc_A(X_AB)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09_G-6jyyxIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator Loss Functions\n",
        "\n",
        "Loss_Disc_A = (tf.reduce_mean(tf.square(Disc_A_real - tf.ones_like(Disc_A_real)) + tf.reduce_mean(tf.square(Disc_A_fake))))/2\n",
        "Loss_Disc_B = (tf.reduce_mean(tf.square(Disc_B_real - tf.ones_like(Disc_B_real)) + tf.reduce_mean(tf.square(Disc_B_fake))))/2\n",
        "\n",
        "Disc_Loss = Loss_Disc_A + Loss_Disc_B"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beVetM5N1xsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# image reconstruction\n",
        "\n",
        "X_BAB = GenAB(X_BA)\n",
        "X_ABA = GenBA(X_AB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmvjYQaKuXpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Generator Loss function\n",
        "Loss_Gen_A = tf.reduce_mean(tf.square(Disc_B_fake - tf.ones_like(Disc_B_fake)))\n",
        "Loss_Gen_B = tf.reduce_mean(tf.square(Disc_A_fake - tf.ones_like(Disc_A_fake)))\n",
        "Loss_total = Loss_Gen_A + Loss_Gen_B\n",
        "\n",
        "# Reconstruction Loss\n",
        "\n",
        "Loss_recon_A = tf.reduce_mean(10*tf.abs((X_A-X_ABA)))\n",
        "Loss_recon_B = tf.reduce_mean(10*tf.abs((X_B-X_BAB)))\n",
        "Loss_recon_total = Loss_recon_A +Loss_recon_B\n",
        "\n",
        "Gen_Loss = Loss_total + Loss_recon_total\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6nBmWh6wLLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters list of Discriminator\n",
        "\n",
        "Disc_param = [Disc_A_W['disc_H'], Disc_A_W[\"disc_final\"], Disc_A_Bias['disc_H'], Disc_A_Bias['disc_final'],\n",
        "              Disc_B_W['disc_H'], Disc_B_W['disc_final'],Disc_B_Bias['disc_H'], Disc_B_Bias['disc_final'] ]\n",
        "# parameters list of Gen\n",
        "Gen_param = [Gen_AB_W['Gen_H'], Gen_AB_W[\"Gen_final\"], Gen_AB_Bias['Gen_H'], Gen_AB_Bias['Gen_final'],\n",
        "            Gen_BA_W['Gen_H'], Gen_BA_W['Gen_final'],Gen_BA_Bias['Gen_H'], Gen_BA_Bias['Gen_final'] ]\n",
        "\n",
        "\n",
        "# Define the optimizer\n",
        "\n",
        "Gen_Optimize = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(Gen_Loss, var_list = Gen_param)\n",
        "Disc_Optimize = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(Disc_Loss, var_list = Disc_param)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq4GKdy8HFt0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "956c3e0e-fc87-4957-ce2d-22ec06608d72"
      },
      "source": [
        "!pip install tensorflow==1.2\n",
        "!pip install -q git+https://github.com/tensorflow/examples.git\n",
        "!ls"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.2 in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.0.1)\n",
            "Requirement already satisfied: backports.weakref==1.0rc1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.0rc1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.18.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (3.10.0)\n",
            "Requirement already satisfied: markdown==2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (2.2.0)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (1.5.0)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2) (0.9999999)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.2) (46.1.3)\n",
            "  Building wheel for tensorflow-examples (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "MNIST_Fashion\t\t     t10k-labels-idx1-ubyte.gz.1\n",
            "sample_data\t\t     train-images-idx3-ubyte.gz\n",
            "t10k-images-idx3-ubyte.gz    train-images-idx3-ubyte.gz.1\n",
            "t10k-images-idx3-ubyte.gz.1  train-labels-idx1-ubyte.gz\n",
            "t10k-labels-idx1-ubyte.gz    train-labels-idx1-ubyte.gz.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-SgrmYoTgiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSHkcV7-EP7z",
        "colab_type": "code",
        "outputId": "b7bb4e50-7212-448d-ac98-be23f5eb4500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!mkdir MNIST_Fashion\n",
        "!cp *.gz MNIST_Fashion/\n",
        "# import input_data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_Fashion/\")\n",
        "\n",
        "# Assembling the training data from 2 domains\n",
        "\n",
        "X_train = mnist.train.images\n",
        "mid = int(X_train.shape[0]/2)\n",
        "\n",
        "# Real Images Dataset 1\n",
        "X_train_real = X_train[:mid]\n",
        "\n",
        "# Rotated images Dataset 2\n",
        "X_train_rot = X_train[mid:].reshape(-1,28,28)\n",
        "X_train_rot = scipy.ndimage.interpolation.rotate(X_train_rot, 90, axes = (1,2))\n",
        "X_train_rot = X_train_rot.reshape(-1,28*28)\n",
        "\n",
        "# random shuffling\n",
        "\n",
        "def shuffle_data(x, size):\n",
        "  start_index = np.random.randint(0, x.shape[0]-size)\n",
        "  return x[start_index:start_index+size]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘MNIST_Fashion’: File exists\n",
            "Extracting MNIST_Fashion/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_Fashion/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_Fashion/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_Fashion/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4NBuJPt2iwm",
        "colab_type": "code",
        "outputId": "e4163636-d964-4246-9a79-a6f8678d922f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# Initialize the variables\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  X_A_batch = shuffle_data(X_train_real, batch_size)\n",
        "  X_B_batch = shuffle_data(X_train_rot, batch_size)\n",
        "\n",
        "  _, Disc_loss_epoch = sess.run([Disc_Optimize, Disc_Loss], feed_dict = {X_A:X_A_batch, X_B:X_B_batch})\n",
        "  _, Gen_loss_epoch = sess.run([Gen_Optimize, Gen_Loss], feed_dict = {X_A:X_A_batch, X_B:X_B_batch})\n",
        "\n",
        "  if epoch % 2000 == 0:\n",
        "    print(\"steps: {0}, Disc loss; {1}, Gen Loss: {2}\".format(epoch, Disc_loss_epoch, Gen_loss_epoch))\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "steps: 0, Disc loss; 0.5391185283660889, Gen Loss: 8.77369213104248\n",
            "steps: 2000, Disc loss; 0.5032532811164856, Gen Loss: 8.50776481628418\n",
            "steps: 4000, Disc loss; 0.45333611965179443, Gen Loss: 8.47620964050293\n",
            "steps: 6000, Disc loss; 0.4041784405708313, Gen Loss: 8.444260597229004\n",
            "steps: 8000, Disc loss; 0.33102962374687195, Gen Loss: 8.41366958618164\n",
            "steps: 10000, Disc loss; 0.315215140581131, Gen Loss: 8.163247108459473\n",
            "steps: 12000, Disc loss; 0.305846244096756, Gen Loss: 8.362942695617676\n",
            "steps: 14000, Disc loss; 0.25823986530303955, Gen Loss: 8.4434814453125\n",
            "steps: 16000, Disc loss; 0.2616346478462219, Gen Loss: 8.125432014465332\n",
            "steps: 18000, Disc loss; 0.2449943721294403, Gen Loss: 8.320176124572754\n",
            "steps: 20000, Disc loss; 0.2664198577404022, Gen Loss: 7.9900803565979\n",
            "steps: 22000, Disc loss; 0.2840080261230469, Gen Loss: 7.727390766143799\n",
            "steps: 24000, Disc loss; 0.2941964268684387, Gen Loss: 7.8364949226379395\n",
            "steps: 26000, Disc loss; 0.27589184045791626, Gen Loss: 7.793934345245361\n",
            "steps: 28000, Disc loss; 0.28468942642211914, Gen Loss: 7.712856769561768\n",
            "steps: 30000, Disc loss; 0.3149760961532593, Gen Loss: 7.47762393951416\n",
            "steps: 32000, Disc loss; 0.2846783697605133, Gen Loss: 7.615700721740723\n",
            "steps: 34000, Disc loss; 0.31562185287475586, Gen Loss: 7.423962593078613\n",
            "steps: 36000, Disc loss; 0.26588743925094604, Gen Loss: 7.506729602813721\n",
            "steps: 38000, Disc loss; 0.29273682832717896, Gen Loss: 7.4584736824035645\n",
            "steps: 40000, Disc loss; 0.26999348402023315, Gen Loss: 7.140913963317871\n",
            "steps: 42000, Disc loss; 0.27885955572128296, Gen Loss: 7.1639604568481445\n",
            "steps: 44000, Disc loss; 0.292624831199646, Gen Loss: 6.93278169631958\n",
            "steps: 46000, Disc loss; 0.31792256236076355, Gen Loss: 6.921602249145508\n",
            "steps: 48000, Disc loss; 0.3166305720806122, Gen Loss: 6.757389068603516\n",
            "steps: 50000, Disc loss; 0.26622074842453003, Gen Loss: 6.898135662078857\n",
            "steps: 52000, Disc loss; 0.27812936902046204, Gen Loss: 6.704930305480957\n",
            "steps: 54000, Disc loss; 0.3344511389732361, Gen Loss: 6.6006178855896\n",
            "steps: 56000, Disc loss; 0.3445402979850769, Gen Loss: 6.368998050689697\n",
            "steps: 58000, Disc loss; 0.36866575479507446, Gen Loss: 6.350350379943848\n",
            "steps: 60000, Disc loss; 0.32153454422950745, Gen Loss: 6.165858268737793\n",
            "steps: 62000, Disc loss; 0.3679540753364563, Gen Loss: 6.22459602355957\n",
            "steps: 64000, Disc loss; 0.3751234710216522, Gen Loss: 6.119558811187744\n",
            "steps: 66000, Disc loss; 0.36272042989730835, Gen Loss: 6.194886684417725\n",
            "steps: 68000, Disc loss; 0.39160212874412537, Gen Loss: 5.884336471557617\n",
            "steps: 70000, Disc loss; 0.40570351481437683, Gen Loss: 6.0460004806518555\n",
            "steps: 72000, Disc loss; 0.41935262084007263, Gen Loss: 6.137434482574463\n",
            "steps: 74000, Disc loss; 0.4210701584815979, Gen Loss: 5.900722026824951\n",
            "steps: 76000, Disc loss; 0.4159492254257202, Gen Loss: 5.841258525848389\n",
            "steps: 78000, Disc loss; 0.44632193446159363, Gen Loss: 5.968878746032715\n",
            "steps: 80000, Disc loss; 0.4311108887195587, Gen Loss: 5.790461540222168\n",
            "steps: 82000, Disc loss; 0.40788525342941284, Gen Loss: 5.937705993652344\n",
            "steps: 84000, Disc loss; 0.427876740694046, Gen Loss: 5.749931335449219\n",
            "steps: 86000, Disc loss; 0.41375893354415894, Gen Loss: 5.870479106903076\n",
            "steps: 88000, Disc loss; 0.4090312123298645, Gen Loss: 5.828136920928955\n",
            "steps: 90000, Disc loss; 0.37651515007019043, Gen Loss: 5.600268840789795\n",
            "steps: 92000, Disc loss; 0.41800788044929504, Gen Loss: 5.63749885559082\n",
            "steps: 94000, Disc loss; 0.38267195224761963, Gen Loss: 5.744230270385742\n",
            "steps: 96000, Disc loss; 0.4113883376121521, Gen Loss: 5.816825866699219\n",
            "steps: 98000, Disc loss; 0.3790174722671509, Gen Loss: 5.633480072021484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad1rGeTg-ypT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing\n",
        "\n",
        "n = 6\n",
        "canvas1 = np.empty((28*n, 28*n))\n",
        "canvas2 = np.empty((28*n, 28*n))\n",
        "\n",
        "for i in range(n):\n",
        "  test_A = shuffle_data(X_train_real, batch_size)\n",
        "  test_B = shuffle_data(X_train_rot, batch_size)\n",
        "\n",
        "  # Generate A images from B\n",
        "  out_A = sess.run(X_BA, feed_dict={X_B: test_B})\n",
        "\n",
        "  # Generate B images from A\n",
        "  out_B = sess.run(X_AB, feed_dict={X_A: test_A})\n",
        "  \n",
        "  for j in range(n):\n",
        "    # Draw the Generated Digits\n",
        "    canvas1[i*28: (i + 1) *28, j* 28:(j+1) *28] = out_A[j].reshape([28,28])\n",
        "\n",
        "  for j in range(n):\n",
        "    # Draw the Generated Digits\n",
        "    canvas2[i*28: (i + 1) *28, j* 28:(j+1) *28] = out_B[j].reshape([28,28])\n",
        "\n",
        "# One way of displaying\n",
        "plt.figure(figsize=(n, n))\n",
        "plt.imshow(canvas1, origin = \"upper\", cmap = \"gray\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize = (n,n))\n",
        "plt.imshow(canvas2, origin=\"upper\", cmap=\"gray\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGvnSGzIKXP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}